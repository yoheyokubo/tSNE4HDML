{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":10131,"status":"ok","timestamp":1716466848258,"user":{"displayName":"Yohei OKUBO","userId":"05862957142098265387"},"user_tz":-540},"id":"oP4LatEHlxdV","outputId":"9cd9e0d6-15f3-416e-bae9-b5e0c82cf998"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting biopython\n","  Downloading biopython-1.83-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from biopython) (1.25.2)\n","Installing collected packages: biopython\n","Successfully installed biopython-1.83\n"]}],"source":["!pip install biopython"]},{"cell_type":"markdown","source":["**Step 1**: Extract taxnomic identifiers (TaxIds) of meta infomation on bacteria and archaea from Phage Host Daily database (https://phdaily.info/). The execution will take about one minute.\n","\n","**Input**. the latest 'data.json' downloaded from the Download tab at the website. Or, we can use the 'data.json' downloaded on March 16th, 2024, which we used in the paper (https://zenodo.org/records/11276021). We assume the 'data.json' is located in the 'data_phd' directory.\n","\n","**Output**. lineage information 'lineage.json' and name information 'scientific_names.json'"],"metadata":{"id":"7sgZGXRKV0f7"}},{"cell_type":"code","source":["import json\n","from os import strerror\n","from textwrap import fill\n","import pandas as pd\n","from collections import defaultdict\n","# loading the downloaded 'data.json'\n","with open('data_phd/data.json') as f:\n","    data_phd = json.load(f)\n","\n","scientific_names = defaultdict(list)\n","lineages = {}\n","assemblies = {}\n","df = pd.DataFrame()\n","\n","def fillin_taxonomydf(entry, entry_id, df, scientific_names, lineages, assemblies):\n","    entry_temp = entry\n","    entry = entry['taxonomy_ncbi']\n","    if entry_id in scientific_names:\n","        return df\n","    if str(entry['lineage'][-1]) != entry_id:\n","        print(f'Something is wrong with the lineage for {entry_id}')\n","        return df\n","    if len(df.index) % 1000 == 0:\n","        print(len(df.index), len(scientific_names))\n","    new_row = {}\n","    new_row['entry_taxid'] = entry_id\n","    new_row['entry_name'] = entry['lineage_names'][-1]\n","    new_row['entry_rank'] = entry['lineage_ranks'][-1]\n","    for taxid, name, rank in zip(reversed(entry['lineage'][:-1]), reversed(entry['lineage_names'][:-1]), reversed(entry['lineage_ranks'][:-1])):\n","        if name != None:\n","            taxid = str(taxid)\n","            new_row[rank] = taxid\n","            if not (taxid in scientific_names and name in scientific_names[taxid]):\n","                scientific_names[taxid].append(name)\n","    if not (entry_id in scientific_names and entry_id in scientific_names[entry_id]):\n","        scientific_names[entry_id].append(new_row['entry_name'])\n","        tail = []\n","        if entry['lineage_names'][0] == 'Viruses':\n","            tail = [None, 1]\n","        elif entry['lineage_names'][0] in ['Bacteria', 'Archaea']:\n","            tail = [131567, 1]\n","        else:\n","            print('{} is not supported!'.format(entry['lineage_names'][0]))\n","        lineages[entry_id] = list(reversed(entry['lineage'])) + tail\n","        if 'assemblies' in entry_temp:\n","            assemblies[entry_id] = entry_temp['assemblies']\n","    return pd.concat([df, pd.DataFrame(new_row, index=[0])], ignore_index=True)\n","\n","for entry in data_phd:\n","    taxid = entry['species_taxid']\n","    name = entry['name']\n","    if taxid in scientific_names and name in scientific_names[taxid]:\n","        print(f'{name} has encountered more than once')\n","    else:\n","        df = fillin_taxonomydf(entry, taxid, df, scientific_names, lineages, assemblies)\n","\n","    for host in entry['hosts']:\n","        if 'lineage_names' in entry['hosts'][host]['taxonomy_ncbi']:\n","            df = fillin_taxonomydf(entry['hosts'][host], host, df, scientific_names, lineages, assemblies)\n","        else:\n","            print(f'{host} for {name} doesnot have lineage')\n","\n","print(len(df.index), len(scientific_names))\n","cnt = 0\n","for taxid in scientific_names:\n","    if len(scientific_names[taxid]) > 1:\n","        cnt += 1\n","print(cnt)\n","assert cnt == 0 # sanity check for data.json\n","scientific_names = {k:v[0] for k,v in scientific_names.items()}\n","print(list(df['entry_rank'].unique())) # the bottom level taxonomic rank\n","print({len(lin) for lin in lineages.values()}) # the length of lineages: species, genus, faimly, order, class, phylum, superkingdom, unknown, unkwon\n","\n","#with open('data_phd/scientific_names.json', 'w') as f:\n","#    json.dump(scientific_names, f, indent=2)\n","#with open('data_phd/lineages.json', 'w') as f:\n","#    json.dump(lineages, f, indent=2)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3OGbYnvDYQH_","executionInfo":{"status":"ok","timestamp":1716466970661,"user_tz":-540,"elapsed":58908,"user":{"displayName":"Yohei OKUBO","userId":"05862957142098265387"}},"outputId":"be43d69b-5ee2-4678-c226-def87f269c33"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["0 0\n","1000 1711\n","2000 3009\n","3000 4181\n","4000 5330\n","5000 6509\n","6000 7616\n","7000 8640\n","8000 9713\n","9000 10787\n","10000 11858\n","11000 12897\n","12000 13948\n","13000 14974\n","14000 15997\n","15000 17006\n","16000 18019\n","17000 19103\n","17709 19834\n","0\n","['species']\n","{9}\n"]}]},{"cell_type":"markdown","source":["**Step 2**: Extract a URL to download complete genome for each species using biopython. Before running, please fill in your email adress so that the curators of NCBI dataabse will warn you if your exceeds a specific (downloading) limit. The execution will take about 12 minutes.\n","\n","**Input**. 'lineage.json'\n","\n","**Output**. 'host_urls.json'"],"metadata":{"id":"KMQ_jnpybgag"}},{"cell_type":"code","source":["your_email = #'AAA@BBB.com'"],"metadata":{"id":"drzM49VUoG5n"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":746465,"status":"ok","timestamp":1716471809765,"user":{"displayName":"Yohei OKUBO","userId":"05862957142098265387"},"user_tz":-540},"id":"RJWgqLVlVdxi","outputId":"15b3febb-ca4e-4ad3-f179-4406dbfdb81a"},"outputs":[{"output_type":"stream","name":"stdout","text":["0.0\n","1.0\n","2.0\n","3.0\n","4.0\n","5.0\n","6.0\n","7.0\n","8.0\n","9.0\n","10.0\n","11.0\n"]}],"source":["from Bio import Entrez\n","import json\n","from collections import defaultdict\n","Entrez.email = your_email\n","#with open('data_phd/lineages.json') as f:\n","#    lineages = json.load(f)\n","id_list = [key for key, lin in lineages.items() if lin[-3] != 10239] # exculde Viruses\n","# id_list = ['666', '562'] for example\n","\n","def get_uids(id_list, assembly_uids, total_counts, assembly_urls, assembly_dics):\n","  # search on taxonomy database\n","  tax_results = Entrez.read(Entrez.epost(db=\"taxonomy\", id=id_list))\n","  webenv = tax_results[\"WebEnv\"]\n","  query_key = tax_results[\"QueryKey\"]\n","  # link the result to assembly database\n","  link_results = Entrez.read(Entrez.elink(dbfrom=\"taxonomy\", db='assembly', linkname='taxonomy_assembly',\\\n","                                    webenv=webenv, query_key=query_key, cmd='neighbor_history'))\n","  webenv2 = link_results[0]['WebEnv']\n","  query_key2 = link_results[0]['LinkSetDbHistory'][0]['QueryKey']\n","  # get statistics by esummary\n","  import re\n","  batch_size = 3000\n","  previous_counts = batch_size\n","  assembly_lens = defaultdict(int)\n","  assembly_srcs = defaultdict(str)\n","  retstart = 0\n","  while previous_counts == batch_size:\n","    record = Entrez.read(Entrez.esummary(db=\"assembly\", webenv=webenv2, query_key=query_key2, retstart=retstart, retmax=batch_size))\n","    previous_counts = len(record['DocumentSummarySet']['DocumentSummary'])\n","    for rec in record['DocumentSummarySet']['DocumentSummary']:\n","      if 'Taxid' in rec and 'Meta' in rec:\n","        taxid = rec['Taxid']\n","        total_len = int(re.search(r'<Stat category=\"total_length\" sequence_tag=\"all\">(\\d+)</Stat>', rec['Meta']).groups()[0])\n","        ungap_len = int(re.search(r'<Stat category=\"ungapped_length\" sequence_tag=\"all\">(\\d+)</Stat>', rec['Meta']).groups()[0])\n","        # extract meta info of complete genome\n","        if taxid in id_list and rec['AssemblyStatus'] == 'Complete Genome' and total_len == ungap_len:\n","          total_counts[taxid] += 1\n","          # RefSeq is preferred to GenBank\n","          if 'RsUid' in rec and len(rec['RsUid']) > 0 and len(rec['ExclFromRefSeq']) == 0:\n","            if assembly_lens[taxid] < total_len or assembly_srcs[taxid] != 'RefSeq':\n","              assembly_lens[taxid] = total_len\n","              assembly_srcs[taxid] = 'RefSeq'\n","              assembly_uids[taxid] = rec.attributes['uid']\n","              assembly_urls[taxid] = rec['FtpPath_RefSeq']\n","              assembly_dics[taxid] = rec\n","          elif 'GbUid' in rec and len(rec['GbUid']) > 0:\n","            if assembly_lens[taxid] < total_len and assembly_srcs[taxid] != 'RefSeq':\n","              assembly_lens[taxid] = total_len\n","              assembly_srcs[taxid] = 'GenBank'\n","              assembly_uids[taxid] = rec.attributes['uid']\n","              assembly_urls[taxid] = rec['FtpPath_GenBank']\n","              assembly_dics[taxid] = rec\n","    retstart += previous_counts\n","\n","assembly_uids = defaultdict(str)\n","assembly_urls = defaultdict(str)\n","assembly_dics = defaultdict(dict)\n","total_counts = defaultdict(int)\n","for pos_start in range(0, len(id_list), 100):\n","  print(pos_start/100)\n","  pos_end = min(pos_start+100, len(id_list))\n","  get_uids(id_list[pos_start:pos_end], assembly_uids, total_counts, assembly_urls, assembly_dics)\n","\n","#with open('data_phd/host_uids.json', 'w') as f:\n","#  json.dump(assembly_uids, f, indent=2)\n","#with open('data_phd/host_counts.json', 'w') as f:\n","#  json.dump(total_counts, f, indent=2)\n","#with open('data_phd/host_urls.json', 'w') as f:\n","#  json.dump(assembly_urls, f, indent=2)\n","#with open('data_phd/host_infos.json', 'w') as f:\n","# json.dump(assembly_dics, f, indent=2)\n","host_urls = assembly_urls"]},{"cell_type":"markdown","source":["**Step 3**: Download whole genome for each species using biopython. The execution will take about 15 minutes.\n","\n","**Input**. 'host_urls.json'\n","\n","**Output**. 'host.fasta' and 'host_accs.json'"],"metadata":{"id":"188oJK7kwQC2"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":899606,"status":"ok","timestamp":1716475285686,"user":{"displayName":"Yohei OKUBO","userId":"05862957142098265387"},"user_tz":-540},"id":"5C5d7cH-ni12","outputId":"a12852ec-182a-4512-e38a-2a29ec7ddc52"},"outputs":[{"output_type":"stream","name":"stdout","text":["0\n","100\n","200\n","300\n","400\n","500\n","600\n"]}],"source":["import urllib\n","import os\n","import gzip\n","import json\n","from collections import defaultdict\n","from Bio import SeqIO\n","def write_fasta(taxid, base_dir, writepath, host_accs):\n","  label = os.path.basename(base_dir)\n","  link = os.path.join(base_dir, label+'_genomic.fna.gz')\n","  data = urllib.request.urlopen(link)\n","  with gzip.open(data, mode='rt') as input:\n","    with open(writepath, 'a' if len(host_accs)>0 else 'w') as output:\n","      records = list(SeqIO.parse(input, \"fasta\"))\n","      SeqIO.write(records, output, 'fasta')\n","      for record in records:\n","        host_accs[taxid].append(record.id)\n","\n","#with open('data_phd/host_urls.json') as f:\n","#    host_urls = json.load(f)\n","host_accs = defaultdict(list)\n","filename = 'data_phd/host.fasta'\n","for i, item in enumerate(host_urls.items()):\n","  taxid, url = item\n","  if i % 100 == 0:\n","    print(i)\n","  if len(url) == 0:\n","    continue\n","  write_fasta(taxid, url, filename, host_accs)\n","#with open('data_phd/host_accs.json', 'w') as f:\n","#  json.dump(host_accs, f, indent=2)"]}],"metadata":{"colab":{"provenance":[{"file_id":"1pP4NhAJxXx7Qj6iUuSQjvQtv8n6I-d9Y","timestamp":1716464250232}],"authorship_tag":"ABX9TyMVqosKhZ2LYevBOgRLnGdK"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
